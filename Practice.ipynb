{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58192eea-795d-471f-899d-0574e1e84fc1",
   "metadata": {},
   "source": [
    "Initial Data Loading and Inspection\r\n",
    "\r\n",
    "Import the dataset using libraries like pandas.\r\n",
    "Perform initial inspection (head, info, describe) to understand the structure, size, and types of dat\n",
    "a.\r\n",
    "Exploratory Data Analysis (EDA)\r\n",
    "\r\n",
    "Data Cleaning: Handling missing values, removing duplicates.\r\n",
    "Text Cleaning: Lowercasing, removing punctuation, removing stop words, stemming/lemmatization.\r\n",
    "Visualization: Word clouds, frequency distributions of words or classes, etc.\r\n",
    "Analyzing the distribution of target classes (sentiment labels) to check for imbalance.\r\n",
    "Basic statistics: Average length of text, most common wo\n",
    "rds, etc.\r\n",
    "Feature Engineering\r\n",
    "\r\n",
    "Text Vectorization: Converting text to numerical format using techniques like Bag-of-Words, TF-IDF.\r\n",
    "Feature Selection: Deciding which features (words or n-grams) to include \n",
    "in the model.\r\n",
    "Data Preprocessing for Modeling\r\n",
    "\r\n",
    "Splitting the data into training and testing sets.\r\n",
    "Further feature scaling or normalizati\n",
    "on, if necessary.\r\n",
    "Model Building\r\n",
    "\r\n",
    "Choosing appropriate models for sentiment analysis (e.g., logistic regression, Naive Bayes, SVM, deep learning models like LSTM).\r\n",
    "Training the models on the training set.\r\n",
    "Hyperparameter tuning u\n",
    "sing cross-validation.\r\n",
    "Model Evaluation\r\n",
    "\r\n",
    "Evaluating the models using appropriate metrics (accuracy, precision, recall, F1-score, ROC-AUC).\r\n",
    "Confusion matrix to understand true positives, false positives, true negatives, and false negatives.\r\n",
    "Model Interpretation\r\n",
    "\r\n",
    "Analyzing the results to understand which features are most important for sentiment prediction.\r\n",
    "Error analysis to understand where and why the model is making errors.\r\n",
    "Model Deployment (Optional)\r\n",
    "\r\n",
    "Deploying the model to a server or integrating it into an existing application.\r\n",
    "Creating an API for the model to make predictions on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b055cd65-d127-4fa2-b44b-e100ae941240",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "* Understanding the Data Structure* \n",
    "\r\n",
    "Data Overview: Use methods like df.head(), df.info(), and df.describe() in pandas to get an overview of the dataset, understand the data types, and identify missing values.\r\n",
    "Identifying Features: Determine which columns are features (e.g., text data) and which is the target variable (e.g., sentiment labels\n",
    "* ).\r\n",
    "Data Clni* ng\r\n",
    "\r\n",
    "Handling Missing Values: Identify and handle missing values in your dataset. You might fill them with a placeholder value or remove the rows/columns with missing data.\r\n",
    "Removing Duplicates: Check for and remove duplicate entries to prevent bias in the \n",
    "* model.\r\n",
    "Text Datareproc* essing\r\n",
    "\r\n",
    "Normalization: Convert text to a consistent format, like lowercasing, to avoid duplication based on case dif* ferences.\r\n",
    "Noise Removal: Remove irrelevant characters, such as punctuation, special characters, and numerical values, which might not be significant in sentiment*  analysis.\r\n",
    "Tokenization: Break down the text into individual words*  or tokens.\r\n",
    "Stop Words Removal: Eliminate common words that may not contribute much to the sentiment (e.g., \"the\", * \"is\", \"at\").\r\n",
    "Stemming/Lemmatization: Reduce words to their base or root form. For example, “running” b\n",
    "\n",
    "** ecomes “run”.\r\n",
    "Text Data Exploration\r\n",
    "\r\n",
    "Word Frequency Analysis: Analyze the most common words or terms in your data. Tools like word clouds can be visually informative.\r\n",
    "Sentiment Distribution: Check the distribution of different sentiment classes (e.g., positive, negative, neutral) to identify any class imbalances that might require addressing.\r\n",
    "Text Length Analysis: Explore the length of the text entries and its distribution. It can be insightful to understand the dataset's verbosity and see if it correla\n",
    "t* es with sentiment.\r\n",
    "Advanced Analysis (Optional)\r\n",
    "\r\n",
    "N-grams Analysis: Explore combinations of N adjacent words (bigrams, trigrams, etc.) to capture more context than single words.\r\n",
    "Correlation Analysis: Examine relationships between text features and sentiment, if applicable.\r\n",
    "Sentiment Over Time: If your dataset is time-based (like tweets), analyzing sentiment trends over \n",
    "* time can be insightful.\r\n",
    "Visualizing the Data\r\n",
    "\r\n",
    "Use graphs like bar charts, histograms, and scatter plots to visualize distributions of text length, word frequencies, and sentiment classes.\r\n",
    "Word clouds for the most frequent words in each sentiment class can prov\n",
    "* ide a quick visual insight.\r\n",
    "Initial Insights and Hypotheses\r\n",
    "\r\n",
    "Based on the EDA, you can formulate hypotheses about your data. For instance, certain words might strongly indicate a particular sentiment.\r\n",
    "Identify potential challenges like class imbalance or a limited vocabulary range that might affect model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff31c1f-d188-4e84-9003-a15b60d74dce",
   "metadata": {},
   "source": [
    "2. Sentiment Distribution\n",
    "Understanding the distribution of different sentiment classes in your dataset is crucial for several reasons:\n",
    "\n",
    "Class Balance: In sentiment analysis, your dataset might have categories like positive, negative, and neutral. It's important to know if these categories are evenly represented. An imbalanced dataset can lead to biased models; for example, if most of your data is positive, the model might be less accurate in identifying negative sentiments.\n",
    "\n",
    "Visualization: You can use bar charts or pie charts to visually represent the proportion of each sentiment class. For example, a bar chart showing the number of instances for each sentiment category (positive, negative, neutral) can quickly reveal imbalances.\n",
    "\n",
    "Strategies for Imbalance: If you find imbalances, you might consider techniques like resampling (either oversampling the minority class or undersampling the majority class), generating synthetic samples (e.g., using SMOTE), or adjusting the class weights in model training.\n",
    "\n",
    "Insights into Data Quality: Sometimes, the distribution of sentiments can reflect on the data collection process. For instance, a dataset scraped from a particular site might be skewed towards positive reviews due to the nature of the site.\n",
    "\n",
    "3. Text Length Analysis\n",
    "Analyzing the length of text entries provides insights into the verbosity and the level of detail in your text data:\n",
    "\n",
    "Length Metrics: Calculate metrics like average length, median length, shortest and longest texts. This helps in understanding the general verbosity in your dataset.\n",
    "\n",
    "Distribution Visualization: Plotting a histogram of text lengths can be very insightful. It shows how text length varies across your dataset and might reveal patterns or outliers. For instance, you might find that most negative sentiments are expressed briefly, while positive feedback tends to be more verbose.\n",
    "\n",
    "Correlation with Sentiment: Sometimes, the length of the text can correlate with the sentiment. Longer texts might be more likely to be detailed reviews (positive or negative), while shorter texts might be simple expressions of satisfaction or dissatisfaction.\n",
    "\n",
    "Impact on Modeling: Knowing about text length can influence preprocessing steps (like deciding on a word limit for each input in neural networks) and can also hint at the need for different feature extraction techniques (like using n-grams to capture more context in shorter texts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05619742-528e-4e26-a160-c155c8b0bde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "#preprocessing\n",
    "\n",
    "import sklearn\n",
    "import nltk\n",
    "import spacy\n",
    "import wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b860804-2af8-4bd0-a7e1-d16cca7232b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\modza\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04b90839-ab6f-42d2-b340-7fb6c3c294e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plotly\n",
      "  Downloading plotly-5.18.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tenacity>=6.2.0 (from plotly)\n",
      "  Downloading tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\modza\\documents\\nlp\\env\\lib\\site-packages (from plotly) (23.1)\n",
      "Downloading plotly-5.18.0-py3-none-any.whl (15.6 MB)\n",
      "   ---------------------------------------- 0.0/15.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/15.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/15.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/15.6 MB 1.5 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 0.7/15.6 MB 5.9 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 1.2/15.6 MB 8.1 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.9/15.6 MB 9.4 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 2.0/15.6 MB 7.9 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 2.2/15.6 MB 7.5 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.9/15.6 MB 8.0 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 3.2/15.6 MB 8.1 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 3.3/15.6 MB 7.6 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 3.6/15.6 MB 7.4 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 3.7/15.6 MB 7.0 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 3.9/15.6 MB 6.8 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 4.2/15.6 MB 6.7 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 4.5/15.6 MB 6.7 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 4.8/15.6 MB 6.7 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 5.1/15.6 MB 6.6 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 5.3/15.6 MB 6.5 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 5.5/15.6 MB 6.4 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 5.6/15.6 MB 6.2 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 5.7/15.6 MB 6.0 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 5.9/15.6 MB 5.9 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 6.1/15.6 MB 5.8 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 6.3/15.6 MB 5.7 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 6.5/15.6 MB 5.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 6.7/15.6 MB 5.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 7.0/15.6 MB 5.7 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 7.3/15.6 MB 5.7 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 7.6/15.6 MB 5.7 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 7.9/15.6 MB 5.7 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 8.2/15.6 MB 5.8 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 8.6/15.6 MB 5.8 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 9.0/15.6 MB 5.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 9.3/15.6 MB 5.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 9.7/15.6 MB 6.0 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 10.2/15.6 MB 6.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 10.5/15.6 MB 6.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 10.9/15.6 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 11.2/15.6 MB 6.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 11.5/15.6 MB 6.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 11.9/15.6 MB 5.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 12.0/15.6 MB 5.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 12.2/15.6 MB 5.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 12.3/15.6 MB 5.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 12.5/15.6 MB 5.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.7/15.6 MB 5.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.9/15.6 MB 5.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 13.1/15.6 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 13.4/15.6 MB 5.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 13.6/15.6 MB 5.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 13.9/15.6 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 14.2/15.6 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.5/15.6 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.9/15.6 MB 5.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.2/15.6 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.6/15.6 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.6/15.6 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.6/15.6 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.6/15.6 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.6/15.6 MB 5.3 MB/s eta 0:00:00\n",
      "Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: tenacity, plotly\n",
      "Successfully installed plotly-5.18.0 tenacity-8.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7feb93b8-9c31-4979-8486-13a135fa8205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.15.0-cp311-cp311-win_amd64.whl.metadata (3.6 kB)\n",
      "Collecting tensorflow-intel==2.15.0 (from tensorflow)\n",
      "  Using cached tensorflow_intel-2.15.0-cp311-cp311-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting h5py>=2.9.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached h5py-3.10.0-cp311-cp311-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached libclang-16.0.6-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting ml-dtypes~=0.2.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached ml_dtypes-0.2.0-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\modza\\documents\\nlp\\env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.26.3)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\modza\\documents\\nlp\\env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading protobuf-4.25.2-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: setuptools in c:\\users\\modza\\documents\\nlp\\env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\modza\\documents\\nlp\\env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\modza\\documents\\nlp\\env\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.9.0)\n",
      "Collecting wrapt<1.15,>=1.11.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached wrapt-1.14.1-cp311-cp311-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached grpcio-1.60.0-cp311-cp311-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting tensorboard<2.16,>=2.15 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached tensorboard-2.15.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.16,>=2.15.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\modza\\documents\\nlp\\env\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.41.2)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading google_auth-2.27.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached google_auth_oauthlib-1.2.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Downloading Markdown-3.5.2-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached protobuf-4.23.4-cp310-abi3-win_amd64.whl.metadata (540 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\modza\\documents\\nlp\\env\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.31.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached werkzeug-3.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\modza\\documents\\nlp\\env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\modza\\documents\\nlp\\env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\modza\\documents\\nlp\\env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\modza\\documents\\nlp\\env\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\modza\\documents\\nlp\\env\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.3)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Using cached tensorflow-2.15.0-cp311-cp311-win_amd64.whl (2.1 kB)\n",
      "Using cached tensorflow_intel-2.15.0-cp311-cp311-win_amd64.whl (300.9 MB)\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "   ---------------------------------------- 0.0/133.7 kB ? eta -:--:--\n",
      "   --------------------------- ------------ 92.2/133.7 kB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 133.7/133.7 kB 2.6 MB/s eta 0:00:00\n",
      "Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Using cached grpcio-1.60.0-cp311-cp311-win_amd64.whl (3.7 MB)\n",
      "Using cached h5py-3.10.0-cp311-cp311-win_amd64.whl (2.7 MB)\n",
      "Using cached keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "Using cached libclang-16.0.6-py2.py3-none-win_amd64.whl (24.4 MB)\n",
      "Using cached ml_dtypes-0.2.0-cp311-cp311-win_amd64.whl (938 kB)\n",
      "Using cached tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n",
      "Using cached protobuf-4.23.4-cp310-abi3-win_amd64.whl (422 kB)\n",
      "Using cached tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached wrapt-1.14.1-cp311-cp311-win_amd64.whl (35 kB)\n",
      "Downloading google_auth-2.27.0-py2.py3-none-any.whl (186 kB)\n",
      "   ---------------------------------------- 0.0/186.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 186.8/186.8 kB 5.5 MB/s eta 0:00:00\n",
      "Using cached google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading Markdown-3.5.2-py3-none-any.whl (103 kB)\n",
      "   ---------------------------------------- 0.0/103.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 103.9/103.9 kB 6.2 MB/s eta 0:00:00\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "Using cached cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Using cached pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, protobuf, opt-einsum, oauthlib, ml-dtypes, markdown, keras, h5py, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, rsa, requests-oauthlib, pyasn1-modules, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 cachetools-5.3.2 flatbuffers-23.5.26 gast-0.5.4 google-auth-2.27.0 google-auth-oauthlib-1.2.0 google-pasta-0.2.0 grpcio-1.60.0 h5py-3.10.0 keras-2.15.0 libclang-16.0.6 markdown-3.5.2 ml-dtypes-0.2.0 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.23.4 pyasn1-0.5.1 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.15.1 tensorboard-data-server-0.7.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 tensorflow-intel-2.15.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.4.0 werkzeug-3.0.1 wrapt-1.14.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mediapipe 0.10.9 requires protobuf<4,>=3.11, but you have protobuf 4.23.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d54e2ab1-6a0f-44d4-9756-9ae2e9956803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\modza\\Documents\\nlp\\env\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pd.options.plotting.backend ='plotly'\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025aca33-190e-4263-b721-61bc6c095d90",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87ba2e49-40c5-4c54-9c0e-b4f370915293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when modi promised “minimum government maximum...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk all the nonsense and continue all the dra...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asking his supporters prefix chowkidar their n...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer who among these the most powerful world...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  category\n",
       "0  when modi promised “minimum government maximum...      -1.0\n",
       "1  talk all the nonsense and continue all the dra...       0.0\n",
       "2  what did just say vote for modi  welcome bjp t...       1.0\n",
       "3  asking his supporters prefix chowkidar their n...       1.0\n",
       "4  answer who among these the most powerful world...       1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Twitter_Data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "080ee0fb-0ccf-4354-ab36-739feb2674c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162980, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c649c6a-7b0e-41a7-a285-6e5eda7648d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 162980 entries, 0 to 162979\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   clean_text  162976 non-null  object \n",
      " 1   category    162973 non-null  float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a09934f0-f157-4014-85bc-f1d3ab15ae29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'when modi promised “minimum government maximum governance” expected him begin the difficult job reforming the state why does take years get justice state should and not business and should exit psus and temples'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.clean_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87536dac-1854-4c3e-9cf6-bb3a8658bb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pandas.io.json import json_normalize\n",
    "from pandas.io.json import _normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07bfc6a6-e791-4987-b4fe-ea1415f3193b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad = pd.read_json('dataset_infos.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48a755a8-0d02-429c-b253-792ab3606b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [0]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_normalize.json_normalize(pd.read_json('dataset_infos.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c71af5d-bed5-480c-b391-cc374d21c168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\modza\\documents\\nlp\\env\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\modza\\documents\\nlp\\env\\lib\\site-packages (from datasets) (1.26.3)\n",
      "Collecting pyarrow>=8.0.0 (from datasets)\n",
      "  Downloading pyarrow-15.0.0-cp311-cp311-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\modza\\documents\\nlp\\env\\lib\\site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\modza\\documents\\nlp\\env\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\modza\\documents\\nlp\\env\\lib\\site-packages (from datasets) (4.65.0)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.4.1-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Using cached multiprocess-0.70.15-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Using cached aiohttp-3.9.1-cp311-cp311-win_amd64.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in c:\\users\\modza\\documents\\nlp\\env\\lib\\site-packages (from datasets) (0.20.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\modza\\documents\\nlp\\env\\lib\\site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\modza\\documents\\nlp\\env\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\modza\\documents\\nlp\\env\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Using cached multidict-6.0.4-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Using cached yarl-1.9.4-cp311-cp311-win_amd64.whl.metadata (32 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Using cached frozenlist-1.4.1-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\modza\\documents\\nlp\\env\\lib\\site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\modza\\documents\\nlp\\env\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\modza\\documents\\nlp\\env\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\modza\\documents\\nlp\\env\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\modza\\documents\\nlp\\env\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Requirement already satisfied: colorama in c:\\users\\modza\\documents\\nlp\\env\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\modza\\documents\\nlp\\env\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\modza\\documents\\nlp\\env\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\modza\\documents\\nlp\\env\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\modza\\documents\\nlp\\env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Using cached datasets-2.16.1-py3-none-any.whl (507 kB)\n",
      "Using cached dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "Using cached fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "Using cached aiohttp-3.9.1-cp311-cp311-win_amd64.whl (364 kB)\n",
      "Downloading pyarrow-15.0.0-cp311-cp311-win_amd64.whl (24.8 MB)\n",
      "   ---------------------------------------- 0.0/24.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.8 MB 991.0 kB/s eta 0:00:25\n",
      "    --------------------------------------- 0.5/24.8 MB 4.9 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 1.2/24.8 MB 8.7 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 1.3/24.8 MB 9.4 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 1.9/24.8 MB 8.3 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 2.4/24.8 MB 8.4 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 2.8/24.8 MB 8.8 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 2.8/24.8 MB 7.9 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 3.3/24.8 MB 7.7 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 3.4/24.8 MB 7.8 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 3.4/24.8 MB 7.8 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 3.4/24.8 MB 7.8 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 3.4/24.8 MB 7.8 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 3.4/24.8 MB 7.8 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 3.4/24.8 MB 7.8 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 3.7/24.8 MB 5.0 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 4.5/24.8 MB 5.7 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 5.2/24.8 MB 6.3 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 5.6/24.8 MB 6.4 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 5.8/24.8 MB 6.3 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 5.9/24.8 MB 6.2 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 6.3/24.8 MB 6.2 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 6.6/24.8 MB 6.2 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 6.8/24.8 MB 6.1 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 7.0/24.8 MB 6.0 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 7.1/24.8 MB 5.9 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 7.2/24.8 MB 5.7 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 7.2/24.8 MB 5.6 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 7.3/24.8 MB 5.4 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 7.4/24.8 MB 5.3 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 7.5/24.8 MB 5.2 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 7.6/24.8 MB 5.1 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 7.8/24.8 MB 5.1 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 7.9/24.8 MB 5.0 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 8.1/24.8 MB 5.0 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 8.3/24.8 MB 5.0 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 8.5/24.8 MB 5.0 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 8.8/24.8 MB 5.0 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 9.0/24.8 MB 5.0 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 9.3/24.8 MB 5.0 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 9.6/24.8 MB 5.0 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 9.9/24.8 MB 5.1 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 10.2/24.8 MB 5.1 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 10.6/24.8 MB 5.2 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 11.0/24.8 MB 5.2 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 11.3/24.8 MB 5.1 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 11.7/24.8 MB 5.2 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 12.2/24.8 MB 5.1 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 12.6/24.8 MB 5.1 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 13.1/24.8 MB 5.2 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 13.4/24.8 MB 5.2 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 13.7/24.8 MB 6.0 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 13.9/24.8 MB 6.0 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 14.2/24.8 MB 5.8 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 14.4/24.8 MB 5.7 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 14.5/24.8 MB 5.6 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 14.7/24.8 MB 5.5 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 14.9/24.8 MB 5.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 15.1/24.8 MB 5.3 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 15.3/24.8 MB 5.2 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 15.5/24.8 MB 5.1 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 15.7/24.8 MB 5.1 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 16.0/24.8 MB 5.0 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 16.3/24.8 MB 5.2 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 16.6/24.8 MB 5.1 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 16.9/24.8 MB 5.2 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 17.3/24.8 MB 5.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 17.6/24.8 MB 5.8 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 18.0/24.8 MB 6.1 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 18.4/24.8 MB 6.3 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 18.8/24.8 MB 6.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 19.2/24.8 MB 6.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 19.5/24.8 MB 6.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 19.7/24.8 MB 6.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 19.8/24.8 MB 6.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 19.9/24.8 MB 6.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 20.1/24.8 MB 6.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 20.2/24.8 MB 6.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 20.4/24.8 MB 6.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 20.6/24.8 MB 6.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 20.8/24.8 MB 5.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 21.0/24.8 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 21.2/24.8 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 21.5/24.8 MB 5.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 21.8/24.8 MB 5.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 22.1/24.8 MB 5.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.4/24.8 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.8/24.8 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.2/24.8 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.5/24.8 MB 5.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.9/24.8 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.3/24.8 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.7/24.8 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.8/24.8 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.8/24.8 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.8/24.8 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.8/24.8 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.8/24.8 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.8/24.8 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.8/24.8 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.8/24.8 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.8/24.8 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.8/24.8 MB 4.7 MB/s eta 0:00:00\n",
      "Using cached multiprocess-0.70.15-py311-none-any.whl (135 kB)\n",
      "Using cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Using cached xxhash-3.4.1-cp311-cp311-win_amd64.whl (29 kB)\n",
      "Using cached frozenlist-1.4.1-cp311-cp311-win_amd64.whl (50 kB)\n",
      "Using cached yarl-1.9.4-cp311-cp311-win_amd64.whl (76 kB)\n",
      "Installing collected packages: xxhash, pyarrow-hotfix, pyarrow, multidict, fsspec, frozenlist, dill, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.12.2\n",
      "    Uninstalling fsspec-2023.12.2:\n",
      "      Successfully uninstalled fsspec-2023.12.2\n",
      "Successfully installed aiohttp-3.9.1 aiosignal-1.3.1 datasets-2.16.1 dill-0.3.7 frozenlist-1.4.1 fsspec-2023.10.0 multidict-6.0.4 multiprocess-0.70.15 pyarrow-15.0.0 pyarrow-hotfix-0.6 xxhash-3.4.1 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b5fb55-c988-47ea-a506-fe0c7432faee",
   "metadata": {},
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"carblacac/twitter-sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2dec9831-7105-43c3-b160-f198f300990a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when modi promised “minimum government maximum...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk all the nonsense and continue all the dra...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asking his supporters prefix chowkidar their n...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer who among these the most powerful world...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162975</th>\n",
       "      <td>why these 456 crores paid neerav modi not reco...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162976</th>\n",
       "      <td>dear rss terrorist payal gawar what about modi...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162977</th>\n",
       "      <td>did you cover her interaction forum where she ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162978</th>\n",
       "      <td>there big project came into india modi dream p...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162979</th>\n",
       "      <td>have you ever listen about like gurukul where ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162980 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               clean_text  category\n",
       "0       when modi promised “minimum government maximum...      -1.0\n",
       "1       talk all the nonsense and continue all the dra...       0.0\n",
       "2       what did just say vote for modi  welcome bjp t...       1.0\n",
       "3       asking his supporters prefix chowkidar their n...       1.0\n",
       "4       answer who among these the most powerful world...       1.0\n",
       "...                                                   ...       ...\n",
       "162975  why these 456 crores paid neerav modi not reco...      -1.0\n",
       "162976  dear rss terrorist payal gawar what about modi...      -1.0\n",
       "162977  did you cover her interaction forum where she ...       0.0\n",
       "162978  there big project came into india modi dream p...       0.0\n",
       "162979  have you ever listen about like gurukul where ...       1.0\n",
       "\n",
       "[162980 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c7aa860-af26-4178-b04b-99dcd155ad04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clean_text    4\n",
       "category      7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d581f6b-cde9-4271-8442-ff49cac8b579",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e43db7a-f1ab-4aba-8221-f09fe5efce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['clean_text']\n",
    "\n",
    "y = df['category']\n",
    "\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d1708a8-4530-4c89-b25d-4f4a111f7303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2419dd13-44e2-4201-a069-8cb4cf81f2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(max_df=0.75, stop_words=&#x27;english&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_df=0.75, stop_words=&#x27;english&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(max_df=0.75, stop_words='english')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english',max_df = 0.75)\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0e2e9683-4441-49be-a4f8-61faa2d03bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "47b69509-710d-4b01-b09f-89ce9d8152b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce9998f5-823c-4979-8870-d23bc24d605f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'d\",\n",
       " \"'ll\",\n",
       " \"'m\",\n",
       " \"'re\",\n",
       " \"'s\",\n",
       " \"'ve\",\n",
       " 'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amount',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'are',\n",
       " 'around',\n",
       " 'as',\n",
       " 'at',\n",
       " 'back',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'both',\n",
       " 'bottom',\n",
       " 'but',\n",
       " 'by',\n",
       " 'ca',\n",
       " 'call',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'could',\n",
       " 'did',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doing',\n",
       " 'done',\n",
       " 'down',\n",
       " 'due',\n",
       " 'during',\n",
       " 'each',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'eleven',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'empty',\n",
       " 'enough',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'except',\n",
       " 'few',\n",
       " 'fifteen',\n",
       " 'fifty',\n",
       " 'first',\n",
       " 'five',\n",
       " 'for',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forty',\n",
       " 'four',\n",
       " 'from',\n",
       " 'front',\n",
       " 'full',\n",
       " 'further',\n",
       " 'get',\n",
       " 'give',\n",
       " 'go',\n",
       " 'had',\n",
       " 'has',\n",
       " 'have',\n",
       " 'he',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'however',\n",
       " 'hundred',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'indeed',\n",
       " 'into',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'keep',\n",
       " 'last',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'made',\n",
       " 'make',\n",
       " 'many',\n",
       " 'may',\n",
       " 'me',\n",
       " 'meanwhile',\n",
       " 'might',\n",
       " 'mine',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'move',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " \"n't\",\n",
       " 'name',\n",
       " 'namely',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'n‘t',\n",
       " 'n’t',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'part',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'please',\n",
       " 'put',\n",
       " 'quite',\n",
       " 'rather',\n",
       " 're',\n",
       " 'really',\n",
       " 'regarding',\n",
       " 'same',\n",
       " 'say',\n",
       " 'see',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'serious',\n",
       " 'several',\n",
       " 'she',\n",
       " 'should',\n",
       " 'show',\n",
       " 'side',\n",
       " 'since',\n",
       " 'six',\n",
       " 'sixty',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhere',\n",
       " 'still',\n",
       " 'such',\n",
       " 'take',\n",
       " 'ten',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " 'thereupon',\n",
       " 'these',\n",
       " 'they',\n",
       " 'third',\n",
       " 'this',\n",
       " 'those',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'together',\n",
       " 'too',\n",
       " 'top',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'twelve',\n",
       " 'twenty',\n",
       " 'two',\n",
       " 'under',\n",
       " 'unless',\n",
       " 'until',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'used',\n",
       " 'using',\n",
       " 'various',\n",
       " 'very',\n",
       " 'via',\n",
       " 'was',\n",
       " 'we',\n",
       " 'well',\n",
       " 'were',\n",
       " 'what',\n",
       " 'whatever',\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'whither',\n",
       " 'who',\n",
       " 'whoever',\n",
       " 'whole',\n",
       " 'whom',\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'would',\n",
       " 'yet',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " '‘d',\n",
       " '‘ll',\n",
       " '‘m',\n",
       " '‘re',\n",
       " '‘s',\n",
       " '‘ve',\n",
       " '’d',\n",
       " '’ll',\n",
       " '’m',\n",
       " '’re',\n",
       " '’s',\n",
       " '’ve'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2efe5d6a-6a97-4280-8257-894c3c610e2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(max_df=0.75, stop_words=&#x27;english&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_df=0.75, stop_words=&#x27;english&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(max_df=0.75, stop_words='english')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b477e3-1f0d-4f28-8ee1-b8eed1928796",
   "metadata": {},
   "source": [
    "tfidf.get_feature_names_out().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b23e58d3-fc08-4210-a4a2-a8184b496b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<122226x89235 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1421153 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_dtm = tfidf.transform(X_train)\n",
    "Xtrain_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "de526be5-2a3f-4fa5-8da8-b401e500f711",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest_dtm = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d63870-7c84-4edd-927d-afc0ddbf4ec8",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e38fa0c8-8552-4e40-823f-14f290283018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196f0628-8a68-40a0-a795-0232e35af0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes.MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1e212ef2-d5e9-40b0-80c0-41f9ba614179",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.naive_bayes.\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ed6e9e55-01bf-4978-bab7-71e7aa7bb6c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = naive_bayes.MultinomialNB()\n",
    "nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b69c7d05-05bd-4682-bc18-10c4c819e621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.fit(Xtrain_dtm,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ea6359da-a836-49b6-b0f8-72a6cbd43641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.predict(Xtest_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4bcf44f9-11ab-4779-a930-37b7de54091c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5694475124561275"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test, nb.predict(Xtest_dtm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f098b86f-1352-40d3-8e62-469fb7605cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['000', '0000', '00000', '000000', '0000000', '00000000',\n",
       "       '000000000', '0000000000', '0000000001', '0001', '0005',\n",
       "       '000kwkajhatka', '000s', '000waiting', '001', '001to', '002',\n",
       "       '007', '007james', '00905523223324', '00s', '00xxxxxxxxx',\n",
       "       '0100am', '0100kph', '0104also', '0118', '015', '018sec',\n",
       "       '01surgical', '02antisatellite', '032019i', '0327', '0339', '034',\n",
       "       '0351', '03it', '03jalgaon', '03x', '0404health', '041', '0414',\n",
       "       '0448pm', '046', '048', '04yrs', '0500', '05042019', '0554',\n",
       "       '05852234246', '05yrs', '06042019', '0607', '0609', '0700',\n",
       "       '070319', '070801z', '07102001', '074223z', '07989964299', '080',\n",
       "       '0800', '080916', '081', '081116', '081127z', '089', '08th', '090',\n",
       "       '0914', '092807z', '0930', '0945', '09555560725', '09999150812',\n",
       "       '09th', '0pposed', '0seats', '0the', '100', '1000', '10000',\n",
       "       '100000', '1000000', '10000000', '1000000000', '1000000000000000',\n",
       "       '1000000000000000000000', '1000000000000000true', '1000000agree',\n",
       "       '1000000k', '100000fine', '100000s', '10000cr', '10000rs',\n",
       "       '10000s', '10000x', '1000100', '1000500', '1000cr', '1000failures',\n",
       "       '1000rs', '1000s', '1000x', '1001', '100100', '1001000', '100150',\n",
       "       '100200', '1003', '100300', '1008', '1008cr', '100agree',\n",
       "       '100backup', '100because', '100better', '100cr', '100crore',\n",
       "       '100days', '100days2014', '100dayscan',\n",
       "       '100electrificaton95sanitation', '100faild', '100in', '100it',\n",
       "       '100k', '100m', '100mbps', '100mistakes', '100modiwhy',\n",
       "       '100molympicsgold', '100mw', '100nonproductive', '100number',\n",
       "       '100p', '100paki', '100ppl', '100right', '100rs', '100rs18gst',\n",
       "       '100s', '100share', '100smart', '100successfull', '100sure',\n",
       "       '100th', '100they', '100thscam', '100times', '100truewe',\n",
       "       '100tweet', '100x', '100years', '100yrs', '101', '1010', '1011',\n",
       "       '1011am', '1012', '1014928997', '1015', '1015mins', '1015yrs',\n",
       "       '1016', '102', '1020', '1020rupeelpg', '1021', '1021lakh',\n",
       "       '102202', '102252z', '10252', '10279', '1029', '103', '1030',\n",
       "       '1030am', '1030pm', '1033', '104', '1040', '1045', '105',\n",
       "       '10500000', '1053', '1054', '10585', '105k', '106', '10600',\n",
       "       '106000', '106000per', '10600year', '1061', '1064', '1067', '1068',\n",
       "       '106800', '106800year', '106pts'], dtype=object)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.get_feature_names_out()[0:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e67aee-6425-4d0d-ba17-a7c8b561af7c",
   "metadata": {},
   "source": [
    "Relationship between Bag of Words and Vectorization Methods\r\n",
    "Bag of Words (BoW) and vectorization methods in Natural Language Processing (NLP) are closely related but are not exactly the same.\r\n",
    "\r\n",
    "BoW is a specific method of representing text data as a collection of words, disregarding grammar and word order but keeping multiplicity (the frequency of each word).\r\n",
    "\r\n",
    "Vectorization is the general process of converting text into numerical feature vectors. This involves several steps, including tokenization, counting, and normalization.\r\n",
    "\r\n",
    "Count Vectorization and BoW are similar in that they both create a representation of a document by counting the frequency of words. However, count vectorization typically creates vectors for the entire vocabulary of the dataset.\r\n",
    "\r\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) Vectorization is another common form of vectorization. Unlike simple count vectorization, TF-IDF also considers the importance of each term in relation to the dataset as a whole.\r\n",
    "\r\n",
    "Sentiment Analysis Project Steps\r\n",
    "For a sentiment analysis project using a dataset from a CSV file, the steps include:\r\n",
    "\r\n",
    "Initial Data Loading and Inspection\r\n",
    "Exploratory Data Analysis (EDA)\r\n",
    "Data Cleaning, Text Cleaning, Visualization, Basic Statistics\r\n",
    "Feature Engineering\r\n",
    "Text Vectorization using techniques like Bag-of-Words, TF-IDF\r\n",
    "Data Preprocessing for Modeling\r\n",
    "Data Splitting, Feature Scaling\r\n",
    "Model Building\r\n",
    "Model Selection, Training, Hyperparameter Tuning\r\n",
    "Model Evaluation\r\n",
    "Accuracy, Precision, Recall, F1-Score, ROC-AUC\r\n",
    "Model Interpretation\r\n",
    "Model Deployment (optional)\r\n",
    "Reporting and Documentation\r\n",
    "Improvement and Iteration\r\n",
    "EDA in Depth for Sentiment Analysis\r\n",
    "Sentiment Distribution: Analyze class balance (positive, negative, neutral) using visualizations like bar charts or pie charts.\r\n",
    "Text Length Analysis: Metrics such as average length, distribution visualization, correlation with sentiment, impact on modeling.\r\n",
    "Feature Selection in Text Data\r\n",
    "Frequency-Based Selection: Select features based on word frequency.\r\n",
    "Mutual Information and Chi-Squared: Measure association between each feature and the target variable.\r\n",
    "TF-IDF: Weigh words based on their importance in the document and across the corpus.\r\n",
    "Machine Learning for Feature Selection: Use models like L1-regularized logistic regression for feature selection.\r\n",
    "Example of Feature Selection using TF-IDF and Chi-Squared\r\n",
    "python\r\n",
    "Copy code\r\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "from sklearn.feature_selection import SelectKBest, chi2\r\n",
    "\r\n",
    "texts = [\"I love this movie\", \"I hate this movie\", \"This is a great movie\", \"This is a bad movie\"]\r\n",
    "labels = [1, 0, 1, 0]  # 1 for positive, 0 for negative\r\n",
    "\r\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 2))\r\n",
    "features = tfidf.fit_transform(texts)\r\n",
    "\r\n",
    "chi2_selector = SelectKBest(chi2, k=2)\r\n",
    "X_kbest = chi2_selector.fit_transform(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd02623-45ca-4692-b52c-dd0f341b8523",
   "metadata": {},
   "source": [
    "from manim import *\n",
    "from IPython.display import Video\n",
    "\n",
    "class ForLoopAnimation(Scene):\n",
    "    def construct(self):\n",
    "        # Create a list to iterate over\n",
    "        elements = [1, 2, 3, 4, 5]\n",
    "        element_texts = [Text(str(element)) for element in elements]\n",
    "\n",
    "        # Position the elements horizontally\n",
    "        self.play(*[Write(element_text) for element_text in element_texts])\n",
    "        self.wait(1)\n",
    "        for element_text in element_texts:\n",
    "            # Highlight the current element\n",
    "            self.play(element_text.animate.set_color(YELLOW))\n",
    "            self.wait(1)\n",
    "            # Return to the original color\n",
    "            self.play(element_text.animate.set_color(WHITE))\n",
    "            self.wait(0.5)\n",
    "\n",
    "        self.wait(2)\n",
    "\n",
    "# Render the animation\n",
    "# Note: We use lower quality (-ql) for faster rendering in Colab\n",
    "!manim -pql -o for_loop_animation.mp4 for_loop_animation.py ForLoopAnimation\n",
    "\n",
    "# Display the rendered video\n",
    "Video(\"for_loop_animation.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509b1619-373e-416b-9642-02d789f26c63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
